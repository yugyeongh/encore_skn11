{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "- 자연어 처리에서 각 문장(문서)의 길이는 다를 수 있음\n",
    "- 그러나 언어모델은 고정된 길이의 데이터를 효율적으로 처리함\n",
    "    - -> 모든 문장의 길이를 동일하게 맞춰주는 작업이 필요함 == 패딩\n",
    "\n",
    "**패딩 이점**\n",
    "1. 일관된 입력 형식\n",
    "2. 병렬 연산 최적화\n",
    "3. 유연한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
    "                        ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
    "                        ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
    "                        ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
    "                        ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 직접 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class TokenizerForPadding:\n",
    "    def __init__(self, num_words=None, oov_token='<OOV>'):\n",
    "        self.num_words = num_words  # 인스턴스 생성 시 전달받음\n",
    "        self.oov_token = oov_token\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.word_counts = Counter()\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        # 빈도수 세기\n",
    "        for sentence in texts:\n",
    "            self.word_counts.update(word for word in sentence if word)  # 개수를 세어주는 word_counts 객체를 업데이트\n",
    "        \n",
    "        # 빈도수 기반 vocabulary  생성 (num_word 만큼만)\n",
    "        # 두 개의 리스트를 합침\n",
    "        vocab = [self.oov_token] + [word for word, _ in self.word_counts.most_common(self.num_words-2 if self.num_words else None)]\n",
    "\n",
    "        self.word_index = {word: i+1 for i, word in enumerate(vocab)}\n",
    "        self.index_word = {i+1: word for word,i in self.word_index.items()}\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [[self.word_index.get(word, self.word_index[self.oov_token]) for word in sentence] for sentence in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0):    # padding, truncating는 pre or post를 가짐\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences) # maxlen은 문장 하나에 들어가있는 토큰(단어)의 개수\n",
    "    \n",
    "    padded_sequences = []\n",
    "    for seq in sequences:   # sequences는 전체 corpus\n",
    "        if len(seq) > maxlen: \n",
    "            if truncating == 'pre':\n",
    "                seq = seq[-maxlen:]\n",
    "            else:\n",
    "                seq = seq[:maxlen]\n",
    "        else: \n",
    "            pad_length = maxlen - len(seq) \n",
    "            if padding == 'pre':\n",
    "                seq = [value] * pad_length + seq\n",
    "            else:\n",
    "                seq = seq + [value]*pad_length\n",
    "        \n",
    "        padded_sequences.append(seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences)\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  2,  6],\n",
       "        [ 0,  0,  0,  0,  2,  9,  6],\n",
       "        [ 0,  0,  0,  0,  2,  4,  6],\n",
       "        [ 0,  0,  0,  0,  0, 10,  3],\n",
       "        [ 0,  0,  0,  3,  5,  4,  3],\n",
       "        [ 0,  0,  0,  0,  0,  4,  3],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  7],\n",
       "        [ 0,  0,  0,  0,  2,  5,  3],\n",
       "        [ 8,  8,  4,  3, 11,  2, 12],\n",
       "        [ 0,  0,  0,  2, 13,  4, 14]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0,  0,  0,  0,  0],\n",
       "        [ 2,  9,  6,  0,  0,  0,  0],\n",
       "        [ 2,  4,  6,  0,  0,  0,  0],\n",
       "        [10,  3,  0,  0,  0,  0,  0],\n",
       "        [ 3,  5,  4,  3,  0,  0,  0],\n",
       "        [ 4,  3,  0,  0,  0,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0,  0,  0],\n",
       "        [ 2,  5,  3,  0,  0,  0,  0],\n",
       "        [ 8,  8,  4,  3, 11,  2, 12],\n",
       "        [ 2, 13,  4, 14,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post')    # padding을 post로 주면 0이 뒤로 감\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0,  0,  0],\n",
       "        [ 2,  9,  6,  0,  0],\n",
       "        [ 2,  4,  6,  0,  0],\n",
       "        [10,  3,  0,  0,  0],\n",
       "        [ 3,  5,  4,  3,  0],\n",
       "        [ 4,  3,  0,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0],\n",
       "        [ 2,  5,  3,  0,  0],\n",
       "        [ 4,  3, 11,  2, 12],\n",
       "        [ 2, 13,  4, 14,  0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  6,  0,  0,  0],\n",
       "        [ 2,  9,  6,  0,  0],\n",
       "        [ 2,  4,  6,  0,  0],\n",
       "        [10,  3,  0,  0,  0],\n",
       "        [ 3,  5,  4,  3,  0],\n",
       "        [ 4,  3,  0,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0],\n",
       "        [ 2,  5,  7,  0,  0],\n",
       "        [ 2,  5,  3,  0,  0],\n",
       "        [ 8,  8,  4,  3, 11],\n",
       "        [ 2, 13,  4, 14,  0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=5, truncating='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 9, 6],\n",
       " [2, 4, 6],\n",
       " [10, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 7],\n",
       " [2, 5, 7],\n",
       " [2, 5, 3],\n",
       " [8, 8, 4, 3, 11, 2, 12],\n",
       " [2, 13, 4, 14]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerForPadding(num_words=15)\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras Tokenizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  0,  1,  8,  5],\n",
       "       [ 0,  0,  0,  0,  1,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  2,  4,  3,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  2],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  2],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 0,  0,  0,  1, 12,  3, 13]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 어린왕자 데이터 샘플 패딩처리\n",
    "\n",
    "1. 텍스트 전처리 (토큰화/불용어처리/정제/정규화)\n",
    "2. 정수 인코딩 Tokenizer (tensorflow.keras)\n",
    "3. 패딩 처리 pad_sequences (tensorflow.keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
