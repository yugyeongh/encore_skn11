{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff47df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í™˜ê²½ì„¤ì •\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8a5ce",
   "metadata": {},
   "source": [
    "### Chainì„ ì´ìš©í•œ Simple LLM\n",
    "1. PromptTemplate\n",
    "2. LLM\n",
    "3. OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb8aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    ('system', 'ë„ˆëŠ” ì• ë‹ˆë©”ì´ì…˜ì„ ì˜ ì•„ëŠ” ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•  ìˆ˜ ìˆì–´.'), \n",
    "    ('user', '{question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1017e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184e7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OutputParser (StrOutputParser)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3b73f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"í¬ì¼“ëª¬ì˜ ê°•í•¨ì€ ì—¬ëŸ¬ ìš”ì†Œì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ì „íˆ¬ ëŠ¥ë ¥, íƒ€ì… ìƒì„±, ê¸°ìˆ  ì¡°í•©, ê·¸ë¦¬ê³  ê°œë³„ í¬ì¼“ëª¬ì˜ ìŠ¤íƒ¯(HP, ê³µê²©ë ¥, ë°©ì–´ë ¥ ë“±)ì— ë”°ë¼ ë‹¬ë¼ì§€ì£ . \\n\\ní˜„ì¬ê¹Œì§€ì˜ í¬ì¼“ëª¬ ê²Œì„ì—ì„œ ê°€ì¥ ê°•ë ¥í•˜ë‹¤ê³  í‰ê°€ë°›ëŠ” í¬ì¼“ëª¬ ì¤‘ í•˜ë‚˜ëŠ” 'ì•„ë¥´ì„¸ìš°ìŠ¤'ì…ë‹ˆë‹¤. ì•„ë¥´ì„¸ìš°ìŠ¤ëŠ” ëª¨ë“  íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆì–´, ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ìœ ë¦¬í•œ ì „íˆ¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\n\\në˜í•œ, 'ë©”ê°€ ë ˆì¿ ì'ë‚˜ 'í”„ë¦¬ì €'ì™€ ê°™ì€ ì „ì„¤ì˜ í¬ì¼“ëª¬ë“¤ë„ ë§¤ìš° ê°•ë ¥í•œ ì„±ëŠ¥ì„ ìë‘í•©ë‹ˆë‹¤. ë©”ê°€ ì§„í™”ê°€ ê°€ëŠ¥í•œ í¬ì¼“ëª¬ë“¤ì€ ì „íˆ¬ì—ì„œ ê°•ë ¥í•œ ìŠ¤íƒ¯ì„ ê°€ì§€ê²Œ ë˜ë¯€ë¡œ, ì „ëµì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nê²°êµ­, ì–´ë–¤ í¬ì¼“ëª¬ì´ ê°€ì¥ ê°•í•œì§€ëŠ” ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë©°, ê°œì¸ì˜ í”Œë ˆì´ ìŠ¤íƒ€ì¼ê³¼ ì „ëµì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • í¬ì¼“ëª¬ì˜ ê°•í•¨ì„ í‰ê°€í•  ë•ŒëŠ” íŒ€ êµ¬ì„±ê³¼ ì „íˆ¬ í™˜ê²½ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Chain -> ì§ˆì˜\n",
    "# question: ì–´ë–¤ í¬ì¼“ëª¬ì´ ê°€ì¥ ê°•í•´?\n",
    "\n",
    "chain = prompt_template | model | output_parser\n",
    "chain.invoke({'question': 'ì–´ë–¤ í¬ì¼“ëª¬ì´ ê°€ì¥ ê°•í•´?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f30068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"í¬ì¼“ëª¬ì˜ ê°•í•¨ì€ ì—¬ëŸ¬ ìš”ì†Œì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ì „íˆ¬ ëŠ¥ë ¥, íƒ€ì… ìƒì„±, ê¸°ìˆ , ê·¸ë¦¬ê³  ì „ëµ ë“±ì´ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. \\n\\ní˜„ì¬ê¹Œì§€ì˜ í¬ì¼“ëª¬ ê²Œì„ì—ì„œ ê°€ì¥ ê°•ë ¥í•˜ë‹¤ê³  í‰ê°€ë°›ëŠ” í¬ì¼“ëª¬ ì¤‘ í•˜ë‚˜ëŠ” **ì•„ë¥´ì„¸ìš°ìŠ¤**ì…ë‹ˆë‹¤. ì•„ë¥´ì„¸ìš°ìŠ¤ëŠ” 'ì‹ ì˜ í¬ì¼“ëª¬'ìœ¼ë¡œ ë¶ˆë¦¬ë©°, ëª¨ë“  íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ ìœ ë¦¬í•œ ì „íˆ¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në˜í•œ, **ë©”ê°€ ë ˆì¿ ìŸˆ**ì™€ **í”„ë¦¬ì¦˜ ë ˆì¿ ìŸˆ**ë„ ë§¤ìš° ê°•ë ¥í•œ í¬ì¼“ëª¬ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ë†’ì€ ìŠ¤íƒ¯ê³¼ ê°•ë ¥í•œ ê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆì–´ ë§ì€ íŠ¸ë ˆì´ë„ˆë“¤ì—ê²Œ ì‚¬ë‘ë°›ê³  ìˆìŠµë‹ˆë‹¤.\\n\\ní•˜ì§€ë§Œ, í¬ì¼“ëª¬ ë°°í‹€ì—ì„œëŠ” ê° í¬ì¼“ëª¬ì˜ íƒ€ì…ê³¼ ìƒëŒ€ í¬ì¼“ëª¬ì˜ ì¡°í•©, ê·¸ë¦¬ê³  íŠ¸ë ˆì´ë„ˆì˜ ì „ëµì´ ìŠ¹íŒ¨ë¥¼ ì¢Œìš°í•˜ê¸° ë•Œë¬¸ì—, ì–´ë–¤ í¬ì¼“ëª¬ì´ ê°€ì¥ ê°•í•œì§€ëŠ” ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ëŒ€íšŒë‚˜ ë©”íƒ€ì—ì„œ ê°•ë ¥í•œ í¬ì¼“ëª¬ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ, ì´ë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "chain = RunnableSequence(prompt_template, model, output_parser)\n",
    "chain.invoke({'question': 'ì–´ë–¤ í¬ì¼“ëª¬ì´ ê°€ì¥ ê°•í•´?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0e7b5",
   "metadata": {},
   "source": [
    "### ë‹¨ê³„ë³„ Chatbot\n",
    "- ì²« ëŒ€í™”ì—ì„œ ë‚´ ì´ë¦„ì„ ì•Œë ¤ì£¼ê³ , ë‹¤ìŒ ëŒ€í™”ì—ì„œ ë‚´ ì´ë¦„ ê¸°ì–µí•˜ëŠ”ì§€ ë¬¼ì–´ë³´ê¸°!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab988462",
   "metadata": {},
   "source": [
    "1. ê·¸ëƒ¥ Chat\n",
    "- langchain_openaiì˜ ChatOPenAI\n",
    "- langchain_core.messageì˜ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f933f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e85216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ìœ ê²½ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([\n",
    "    HumanMessage('ì•ˆë…• ë‚´ ì´ë¦„ì€ ìœ ê²½ì´ì•¼')\n",
    "]).content      # ëª¨ë¸ì˜ ì‘ë‹µ ì¤‘ ë©”ì‹œì§€ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291c46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì œê°€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì´ë¦„ì„ ì•Œê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì´ë¦„ì„ ì•Œë ¤ì£¼ì‹œë©´ ê·¸ì— ë§ì¶° ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ê±´ìœ¼ë¡œ ì§ˆì˜ë¥¼ ë³´ë‚´ê¸° ë•Œë¬¸ì— ë‹¨ìˆœí•œ ì±„íŒ…ì´ ë¨\n",
    "model.invoke([\n",
    "    HumanMessage('ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì§€?')\n",
    "]).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dcda4f",
   "metadata": {},
   "source": [
    "2. ì§ì „ ëŒ€í™” ë§¥ë½ ìœ ì§€\n",
    "- langchain_openaiì˜ ChatOpenAI\n",
    "- langchain_core.messagesì˜ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92526061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’ğŸ»â€â™€ï¸ ì•ˆë…• ë‚´ ì´ë¦„ì€ ìœ ê²½ì´ì•¼\n",
      "ğŸ¤– ì•ˆë…•í•˜ì„¸ìš”, ìœ ê²½ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "ğŸ’ğŸ»â€â™€ï¸ ë‚´ ì´ë¦„ì´ ë­ë¼êµ¬?\n",
      "ğŸ¤– ìœ ê²½ë‹˜ì´ë¼ê³  í•˜ì…¨ìŠµë‹ˆë‹¤. ë§ë‚˜ìš”?\n",
      "ğŸ’ğŸ»â€â™€ï¸ ì—‰ ë§ì•„!\n",
      "ğŸ¤– ì¢‹ì•„ìš”, ìœ ê²½ë‹˜! ì˜¤ëŠ˜ì€ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = []\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"ğŸ‘¤ \") \n",
    "    if prompt == 'ë':\n",
    "        break\n",
    "\n",
    "    user_message = HumanMessage(content=prompt)\n",
    "    messages.append(user_message)\n",
    "    print(f'ğŸ’ğŸ»â€â™€ï¸ {user_message.content}')\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    assistant_message = AIMessage(content=response.content)\n",
    "    messages.append(assistant_message)\n",
    "    print(f'ğŸ¤– {assistant_message.content}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90571e",
   "metadata": {},
   "source": [
    "3. Memoryë¡œ ëŒ€í™”ì˜ ë§¥ë½ ìœ ì§€\n",
    "- langchain_openaiì˜ ChatOpenAI\n",
    "- langchain_core.messagesì˜ í´ë˜ìŠ¤\n",
    "- langchain_core.runnablesì˜ í´ë˜ìŠ¤\n",
    "- langchain_core.promptsì˜ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f88c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’ğŸ»â€â™€ï¸ ì•ˆë…• ë‚˜ëŠ” ìœ ê²½ì´ì•¼!\n",
      "ğŸ¤– ì•ˆë…•í•˜ì„¸ìš”, ìœ ê²½ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”! ì˜¤ëŠ˜ì€ ì–´ë–¤ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ ë³¼ê¹Œìš”? ğŸ˜Š\n",
      "ğŸ’ğŸ»â€â™€ï¸ ë„ˆê°€ ë¨¼ì € ì¬ë°ŒëŠ” ì£¼ì œë¥¼ êº¼ë‚´ì¤˜\n",
      "ğŸ¤– ì¢‹ì•„ìš”! ê·¸ëŸ¼ ìš”ì¦˜ ì‚¬ëŒë“¤ ì‚¬ì´ì—ì„œ ì¸ê¸° ìˆëŠ” ì£¼ì œ ì¤‘ í•˜ë‚˜ì¸ 'ì—¬í–‰'ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³¼ê¹Œìš”? ìœ ê²½ë‹˜ì€ ì–´ë–¤ ì—¬í–‰ì§€ë¥¼ ê°€ì¥ ê°€ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ì•„ë‹ˆë©´ ê¸°ì–µì— ë‚¨ëŠ” ì—¬í–‰ ê²½í—˜ì´ ìˆë‹¤ë©´ ê³µìœ í•´ì£¼ì‹¤ë˜ìš”? ğŸŒâœˆï¸\n",
      "ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory):\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def add_messages(self, messages):\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.messages)\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'ë„ˆëŠ” {skill}ì„ ê°€ì§„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.'), \n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    ('human', '{query}')\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.5)\n",
    "chain = prompt | model\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_by_session_id,\n",
    "    input_messages_key='query',\n",
    "    history_messages_key='history'\n",
    ")\n",
    "\n",
    "session_id = 'hyg'\n",
    "skill = 'ESFJì˜ ì„±í–¥'\n",
    "\n",
    "while True:\n",
    "    query = input(\"ğŸ‘¤ ìœ ì €: \")\n",
    "    if query == 'ë':\n",
    "        print(\"ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    print(f'ğŸ’ğŸ»â€â™€ï¸ {query}')\n",
    "\n",
    "    response = chain_with_history.invoke(\n",
    "        {'skill': skill, 'query': query},\n",
    "        config={'configurable': {'session_id': session_id}}\n",
    "    )\n",
    "    print(f\"ğŸ¤– {response.content}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6542863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "prompt - ChatPromptTemplate([\n",
    "    SystemMessage('ë‹¹ì‹ ì€ ëª¸ì‹œ ì¹œì ˆí•˜ê³  ìƒëƒ¥í•œ ì¹œê·¼í•œ ì±—ë´‡ì…ë‹ˆë‹¤.'),\n",
    "    MessagesPlaceholder('history'),\n",
    "    HumanMessage('{query}')\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-4o-mini', tenoerature=0.5)\n",
    "chain = prompt | model\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_by_session_id,\n",
    "    input_messages_key='query',\n",
    "    history_messages_key='history'\n",
    ")\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "        {'query': 'ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì§€?'},\n",
    "        config={'configurable': {'session_id': 'apple'}}\n",
    ")\n",
    "\n",
    "# ì±„íŒ… ë‚´ì—­ ë½‘ì•„ë³´ê¸°\n",
    "for message in store['apple'].messages:\n",
    "    if isinstance(message, HumanMessage):        # isinstance: ê°ì²´ê°€ íŠ¹ì • í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ì§€ ì•„ë‹Œì§€\n",
    "        print(f'ğŸ: {message.content}')\n",
    "    elif isinstance(message, AIMessage):\n",
    "        print(f'ğŸ¤–: {message.content}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
