{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d917841e",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training,PeftModel\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 토큰 가져오기\n",
    "\n",
    "load_dotenv()\n",
    "hf_token=os.getenv('HF_TOKEN')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da5a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c01cd7",
   "metadata": {},
   "source": [
    "## DPO Flow \n",
    "\n",
    "- 개념\n",
    "    - DPO는 사용자의 `선호도 피드백을 활용`해 언어 모델을 튜닝하는 방법\n",
    "    - 전통적인 RLHF 기법(PPO 기반 보상 모델 학습 후 정책 업데이트)과 달리 보상 모델 없이 선호/비선호 응답 쌍으로 선호 응답이 비선호 응답보다 더 높은 확률을 갖도록 학습\n",
    "    - PPO 보다 단순하고 안정적으로 작동해 계산 비용이 적음 \n",
    "- 학습 과정\n",
    "    - 4비트 양자화 설정하기 > `BitsAndBytesConfig`\n",
    "    - 모델에 양자화 설정을 적용하기 > `AutoModelForCausalLM`\n",
    "    - 토크나이저 로드 > `AutoTokenizer`\n",
    "        - tokenizer.pad_token is None -> pad_token = eos_token 설정하기 (설정하지 않으면 오류 발생 가능성이 있음)\n",
    "    - LoRA 설정 > `LoraConfig`\n",
    "    - 모델 준비 > `prepare_model_for_kbit_training` & `get_peft_model`\n",
    "    - 데이터셋 로드 > `load_dataset`\n",
    "    - 데이터 전처리 함수 만들기 > `tokenizer` 사용해 `prepare_text` 함수 만들기 - 토크나이저를 사용해 텐서화\n",
    "    - 데이터 토큰화 및 Pytorch 텐서로 형식 변환 > `prepare_text` 함수 사용 & `set_format`\n",
    "    - 데이터 배치 구성 함수 만들기 > `torch.stack` 사용해 데이터 배치 구성을 수행하는 `collate_fn` 함수 만들기 - 토크나이저 반환\n",
    "    - DPO 트레이너 클래스 구성 > `Trainer` 모듈의 `compute_loss 메서드` 오버라이딩 > 선호/비선호 응답에 대한 `DPO 손실 공식`을 이용해 loss 계산\n",
    "    - 훈련 설정: TrainingArguments 만들기 > `TrainingArguments`\n",
    "        - 학습에 필요한 설정 (batch size, epoch 수, 저장 디렉토리 등)\n",
    "    - 훈련 과정 정의: `DPOTrainer`\n",
    "        - 선호/비선호 응답 기반 손실 계산 및 모델 업데이트 로직 담당\n",
    "    - 학습: trainer.train()\n",
    "        - 학습 루프 시작 (args에 따라 배치 구성, 손실 계산, 옵티마이저 적용 수행)\n",
    "- 학습된 모델 사용\n",
    "    - checkpoint path 불러오기 및 eval > `model.eval()` : 추론 모드 전환\n",
    "    - 응답 만들어주는 함수\n",
    "        - `tokenizer`: input 토큰화하기\n",
    "        - `torch.no_grad()`: 추론 시 학습하지 않으므로 grad 연산 제거하기\n",
    "        - `decode`: 숫자를 사람이 읽을 수 있는 문자로 변경해 리턴\n",
    "    - 응답 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c041a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Bllossom/llama-3.2-Korean-Bllossom-3B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4비트 양자화 설정하기\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # 4비트 양자화 설정 on\n",
    "    bnb_4bit_quant_type='nf4',              # 4비트 양자화 방식 설정\n",
    "    bnb_4bit_use_double_quant=True,         # 두번 양자화하는거 사용 여부\n",
    "    bnb_4bit_compute_dtype=torch.float16    # 연산 시의 데이터 타입\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31036ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit 양자화를 적용한 모델 로드\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 로드\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # lora 차원 수\n",
    "    lora_alpha=32,          # lora 스케일링 개수\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj'],   # 각각의 가중치 행렬들이 로라에 적용할 대상 모델이 됨                       # q_proj: query: 입력 데이터를 쿼리 벡터로 변환, key: 입력 데이터를 키 벡터로 변환, value: 입력 데이터를 값 벡터로 변환, o_proj: output: 출력값의 원래 차원으로 변환하는 가중치 행렬로, gate: 입력 벡터가 선형 변환을 수행한 과정을 거친 가중치 행렬, down: mlp에서 고차원 벡터를 낮은 차원으로 변환, up: 낮은 차원의 벡터를 원래 벡터로 확장하는 가중치 행렬\n",
    "    bias='none',            #\n",
    "    task_type='CAUSAL_LM'   # causal_lm: gpt 계열의 모델로 task_type 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 준비\n",
    "# 로라 학습에 적합하게 불러온 모델을 준비\n",
    "\n",
    "#  prepare_model_for_kbit_training\n",
    "#  4비트로 압축한 모델을 안전하고 효율적으로 finetune할 수 있도록 “훈련 가능한 구조”로 바꿔주는 함수\n",
    "# 함수가 하는 일: 양자화 레이어 유지, 필요한 레이어만 float16, float32로 변환\n",
    "model = prepare_model_for_kbit_training(model)  # 양자화된 모델을 학습 가능하게 변환\n",
    "model = get_peft_model(model, lora_config)          # lora 어댑터를 적용한 모델 만들기\n",
    "\n",
    "model.print_trainable_parameters()                  # 학습 가능한 파라미터 출력\n",
    "model.train()                                       # 학습 모드 전환: 모델은 기본적으로 평가 모드로 훈련되기 때문에 학습 모드로 훈련될 수 있게 변경해줘야함\n",
    "model.gradient_checkpointing_enable()               # gpu 메모리 사용량을 줄이기 위해 역전파 과정에서 중간 결과를 저장하지 않고 바로 다시 중간결과를 계산하는 방식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "\n",
    "dataset = load_dataset('mncai/orca_dpo_pairs_ko')       # 언어 모델 능력 향상을 위해 만들어 놓은 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "\n",
    "# 텍스트 모델이 이해할 수 있도록 토큰화하고 숫자 인덱스로 변환\n",
    "def preprocess_text(sample):\n",
    "    input_enc=tokenizer(sample['question'], padding='max_length', max_length=256, truncation=True),\n",
    "    preferred_enctokenizer(sample['chosen'], padding='max_length', max_length=256, truncation=True)\n",
    "    dispreferred_enc=tokenizer(sample['rejected'], padding='max_length', max_length=256, truncation=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_enc['input_ids'],\n",
    "        'attention_mask': input_enc['attention_mask'],\n",
    "        'preferred_ids': preferred_enc['input_ids'],\n",
    "        'dispreferred_ids': dispreferred_enc['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dc12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 토큰화 및 pytorch 텐서 형식으로 변환\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "tokenized_dataset = dataset['train'].map(\n",
    "    preprocess_text,\n",
    "    remove_columns=['id', 'system', 'question', 'chose', 'rejected']    # 데이터셋에서 불필요한 정보 삭제\n",
    ")\n",
    "\n",
    "# pytorch 텐서로 형식 변환\n",
    "# 받아온 dataset은 허깅페이스 데이터셋이라 pytorch 모델에서 학습할 수 있도록 형태를 변환\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'preferred_ids', 'dispreferred_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84011555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 배치 구성 함수\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack(item['input_ids'].clone().detach() for item in batch)\n",
    "    attention_mask = torch.stack(item['attention_mask'].clone().detach() for item in batch)\n",
    "\n",
    "    # max(len(item['preferred_ids']): 선호 응답으로 되어 있는 아이 중 맥스 값 중 \n",
    "    # max(max(len(item['preferred_ids']) for item in batch), 1): 잘못 들어온 케이스에 대비해 최대 길이를 1로 설정\n",
    "    max_length = max(max(len(item['preferred_ids']) for item in batch), 1)\n",
    "\n",
    "    # 선호 응답\n",
    "    # 패딩 처리 후 배치 처리\n",
    "    preferred_ids = torch.stack([\n",
    "        torch.tensor(\n",
    "            # 패딩 처리: 최대 길이만큼 토큰으로 채워주기\n",
    "            item['preferred_ids'].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['preferred_ids'])), \n",
    "            dtype=torch.long\n",
    "        ) if isinstance(item['preferred_ids'], torch.tensor) \n",
    "        else torch.tensor(\n",
    "            item['preferred_ids'].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['preferred_ids'])), \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        for item in batch\n",
    "    ]).clone().detach()     # 배치 구성될 수 있게 clone.detach\n",
    "\n",
    "    # 비선호 응답\n",
    "    # 패딩 처리 후 배치 처리\n",
    "    dispreferred_ids = torch.stack([\n",
    "        torch.tensor(\n",
    "            item['dispreferred_ids'].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['dispreferred_ids'])), \n",
    "            dtype=torch.long\n",
    "        ) if isinstance(item['dispreferred_ids'], torch.tensor) \n",
    "        else torch.tensor(\n",
    "            item['dispreferred_ids'].tolist() + [tokenizer.pad_token_id] * (max_length - len(item['dispreferred_ids'])), \n",
    "            dtype=torch.long\n",
    "        )\n",
    "        for item in batch\n",
    "    ]).clone().detach()\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'preferred_ids': preferred_ids,\n",
    "        'dispreferred_ids': dispreferred_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer(Trainer):\n",
    "    # 선호, 비선호 응답에 대한 loss 구하기\n",
    "    def compute_loss(self, model, inputs, beta=0.1, *args, **kwargs):\n",
    "        # 연산을 할 수 있게 device(장치)를 맞춰줌\n",
    "        input_ids=inputs['input_ids'].to(model.device),\n",
    "        attention_mask=inputs['attention_mask'].to(model.device),\n",
    "        preferred_ids=inputs['preferred_ids'].to(model.device),\n",
    "        dispreferred_ids=inputs['dispreferred_ids'].to(model.device)\n",
    "\n",
    "        # 선호/비선호 응답에 대한 출력 구하기\n",
    "        preferred_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=preferred_ids)\n",
    "        dispreferred_outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=dispreferred_ids)\n",
    "\n",
    "        # 출력에서 손실값 구하기\n",
    "        preferred_loss = preferred_outputs.loss\n",
    "        dispreferred_loss = dispreferred_outputs.loss\n",
    "\n",
    "        # DPO 손실의 공식을 적용하기\n",
    "        # import torch.nn.functional as F\n",
    "        loss = -F.logsigmoid(beta * (dispreferred_loss - preferred_loss)).mean()    # 선호할 응답의 확률이 비선호 응답의 확률보다 높아지게 학습을 유도해서 DPO의 손실 공식을 적용\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 설정: 트레이닝 arguments 만들기\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./dpo_llama3_korean',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    save_strategy='steps',\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    optim='adamw_bnb_8bit',\n",
    "    max_grad_norm=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 과정 정의: DPOTrainer 클래스 사용해 trainer 객체 만들기\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 - DPO  기반의 파인튜닝 학습 끝\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435b6d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 사용하기\n",
    "\n",
    "checkpoint_path = './dpo_llama3_korean/checkpoint-xxx'\n",
    "\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "model.eval()        # 추론 모드 전환: 입력을 주고 출력을 예측할 때 추론 모드라고 설정\n",
    "\n",
    "sample_data = dataset['train'].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답 만들어주기\n",
    "\n",
    "def generate_response(question):\n",
    "    # 들어오는 인풋은 토큰화해줘야함\n",
    "    inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True, max_length=256).to(model.device)\n",
    "\n",
    "    # 추론 모드에서 미분(gradient)이 계산되지 않게 걸어줌 -> 추론 시에는 학습하지 않으므로 불필요한 연산을 제거해준것\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccf594",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(sample_data):\n",
    "    question = example['question']\n",
    "    preferred_answer = example['chosen']\n",
    "\n",
    "    generated_response = generated_response(question)\n",
    "\n",
    "    print(f'질문: {question}')\n",
    "    print(f'정답 (선호응답): {preferred_answer}')\n",
    "    print(f'실제 모델 응답: {generated_response}')\n",
    "    print('-'*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
